import os
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# --- é…ç½® ---
PDF_PATH = "data/MAT235Y12025-26Syllabus.pdf"  # è¯·ç¡®ä¿æ–‡ä»¶åå’Œè·¯å¾„æ­£ç¡®
DB_PATH = "./chroma_db_web"     # è¿™é‡Œæ¢ä¸ªæ–°åå­—ï¼Œé˜²æ­¢å’Œä¹‹å‰çš„å†²çª
LLM_MODEL = "llama3"
EMBED_MODEL = "nomic-embed-text" # è®°å¾—ä¸€å®šè¦ç”¨è¿™ä¸ªï¼

# --- RAG å¼•æ“ç±» (å°è£…é€»è¾‘) ---
class RAGService:
    def __init__(self):
        self.vector_store = None
        self.retriever = None
        self.chain = None
        self.initialize_rag()

    def initialize_rag(self):
        print("ğŸš€ [Backend] æ­£åœ¨åˆå§‹åŒ– RAG å¼•æ“...")
        
        # 1. æ¨¡å‹åˆå§‹åŒ–
        embeddings = OllamaEmbeddings(model=EMBED_MODEL)
        llm = ChatOllama(model=LLM_MODEL)

        # 2. æ£€æŸ¥å¹¶å»ºç«‹å‘é‡åº“
        if not os.path.exists(DB_PATH):
            print(f"ğŸ“„ [Backend] æœªå‘ç°æ•°æ®åº“ï¼Œæ­£åœ¨å¤„ç† PDF: {PDF_PATH}...")
            if not os.path.exists(PDF_PATH):
                raise FileNotFoundError(f"æ‰¾ä¸åˆ° PDF æ–‡ä»¶: {PDF_PATH}")
            
            loader = PyPDFLoader(PDF_PATH)
            docs = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            splits = text_splitter.split_documents(docs)
            
            self.vector_store = Chroma.from_documents(
                documents=splits,
                embedding=embeddings,
                persist_directory=DB_PATH
            )
            print("ğŸ’¾ [Backend] å‘é‡åº“å»ºç«‹å®Œæˆï¼")
        else:
            print("ğŸ’¾ [Backend] åŠ è½½å·²æœ‰å‘é‡åº“...")
            self.vector_store = Chroma(
                persist_directory=DB_PATH,
                embedding_function=embeddings
            )

        # 3. è®¾ç½®æ£€ç´¢å™¨
        self.retriever = self.vector_store.as_retriever(search_kwargs={"k": 5})

        # 4. è®¾ç½® Prompt å’Œ Chain
        template = """
        You are an intelligent teaching assistant. Answer the student's question based ONLY on the context below.
        If the answer is not in the context, say "I cannot find this information in the syllabus."
        
        Context:
        {context}
        
        Question:
        {question}
        """
        prompt = ChatPromptTemplate.from_template(template)
        
        self.chain = (
            {"context": self.retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        print("âœ… [Backend] ç³»ç»Ÿå°±ç»ªï¼")

    def get_answer(self, question: str):
        if not self.chain:
            return "System not initialized."
        return self.chain.invoke(question)

# --- FastAPI App è®¾ç½® ---
app = FastAPI()

# å…è®¸è·¨åŸŸ (CORS)ï¼Œè¿™æ ·ä½ çš„ HTML æ–‡ä»¶æ‰èƒ½ç›´æ¥è®¿é—®è¿™ä¸ª API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # å…è®¸æ‰€æœ‰æ¥æº
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€åˆå§‹åŒ– RAG æœåŠ¡ (å¯åŠ¨æ—¶åªè¿è¡Œä¸€æ¬¡)
rag_service = RAGService()

# å®šä¹‰è¯·æ±‚æ•°æ®ç»“æ„
class QueryRequest(BaseModel):
    question: str

# å®šä¹‰ API æ¥å£
@app.post("/chat")
async def chat_endpoint(request: QueryRequest):
    try:
        print(f"ğŸ“© æ”¶åˆ°é—®é¢˜: {request.question}")
        answer = rag_service.get_answer(request.question)
        return {"answer": answer}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# å¯åŠ¨å‘½ä»¤: uvicorn main:app --reload