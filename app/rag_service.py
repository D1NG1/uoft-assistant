import os
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from app.config import PDF_DIR, PDF_FILES, DB_PATH, LLM_MODEL, EMBED_MODEL


class RAGService:
    """RAG (Retrieval-Augmented Generation) æœåŠ¡ç±»"""

    def __init__(self):
        self.vector_store = None
        self.retriever = None
        self.chain = None
        self.initialize_rag()

    def initialize_rag(self):
        """åˆå§‹åŒ– RAG å¼•æ“"""
        print("ğŸš€ [Backend] æ­£åœ¨åˆå§‹åŒ– RAG å¼•æ“...")

        # 1. æ¨¡å‹åˆå§‹åŒ–
        embeddings = OllamaEmbeddings(model=EMBED_MODEL)
        llm = ChatOllama(model=LLM_MODEL)

        # 2. æ£€æŸ¥å¹¶å»ºç«‹å‘é‡åº“
        if not os.path.exists(DB_PATH):
            print(f"ğŸ“„ [Backend] æœªå‘ç°æ•°æ®åº“ï¼Œæ­£åœ¨å¤„ç† PDF æ–‡ä»¶...")
            self._load_pdfs_to_vector_store(embeddings)
        else:
            print("ğŸ’¾ [Backend] åŠ è½½å·²æœ‰å‘é‡åº“...")
            self.vector_store = Chroma(
                persist_directory=DB_PATH,
                embedding_function=embeddings
            )

        # 3. è®¾ç½®æ£€ç´¢å™¨
        self.retriever = self.vector_store.as_retriever(search_kwargs={"k": 5})

        # 4. è®¾ç½® Prompt å’Œ Chain
        template = """
        You are an intelligent teaching assistant. Answer the student's question based ONLY on the context below.
        If the answer is not in the context, say "I cannot find this information in the syllabus."

        Context:
        {context}

        Question:
        {question}
        """
        prompt = ChatPromptTemplate.from_template(template)

        self.chain = (
            {"context": self.retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        print("âœ… [Backend] ç³»ç»Ÿå°±ç»ªï¼")

    def _load_pdfs_to_vector_store(self, embeddings):
        """åŠ è½½æ‰€æœ‰ PDF æ–‡ä»¶åˆ°å‘é‡æ•°æ®åº“"""
        all_splits = []

        for pdf_file in PDF_FILES:
            pdf_path = PDF_DIR / pdf_file
            if not pdf_path.exists():
                print(f"âš ï¸ [Backend] è­¦å‘Š: æ‰¾ä¸åˆ°æ–‡ä»¶ {pdf_path}")
                continue

            print(f"ğŸ“„ [Backend] å¤„ç† PDF: {pdf_file}...")
            loader = PyPDFLoader(str(pdf_path))
            docs = loader.load()

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            splits = text_splitter.split_documents(docs)
            all_splits.extend(splits)

        if not all_splits:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ä»»ä½• PDF æ–‡ä»¶åœ¨ {PDF_DIR}")

        print(f"ğŸ“Š [Backend] å…±å¤„ç† {len(all_splits)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        self.vector_store = Chroma.from_documents(
            documents=all_splits,
            embedding_function=embeddings,
            persist_directory=DB_PATH
        )
        print("ğŸ’¾ [Backend] å‘é‡åº“å»ºç«‹å®Œæˆï¼")

    def get_answer(self, question: str) -> str:
        """è·å–é—®é¢˜çš„ç­”æ¡ˆ"""
        if not self.chain:
            return "System not initialized."
        return self.chain.invoke(question)
